---
title: "An overview"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We have provided a general overview of `modelevaluation.org` and described the function of each major component. We also recommend taking a look at our [step-by-step case study]()

## Access

To begin using `modelevaluation.org`, users must first register for an account. 
Click on `Register` on the top right hand corner and fill in the relevant information. Note that users must agree to the [Terms of Use](https://modelevaluation.org/terms-of-use) to create an account. Once registration is complete, click on  `Login` and enter your credentials to access `modelevaluation.org`. If you would like to have admin user rights, please contact us [here](). 

## The Workflow

`modelevaluation.org` facilitates model comparisons by hosting model comparison experiments and their respective data sets, by providing a centralized space for models to be evaluated objectively. A registered user can download the data for a particular experiment, run their model simulation locally and upload their model output to `modelevaluation.org` see how their model performed relative to other models and with the original data?[not sure what is the best wording here] in the same experiment. 

## Workspace

Any user can create a workspace in the web application. A workspace is an environment for model development and is used to group users together that are collaborating on the same project. A workshop stores the necessary [experiment](#experiment) and [datasets](#data) for a particular model comparison project. A workspace can be used privately or can be made public for larger scale model inter-comparisons. [Want to link to a relevant paper]

## Experiment {#experiment}

An experiment sets the key parameters (e.g data, types of analyses, spatial scale) for a model comparison project. It contains a [master analysis script](#analysisscript) which determines what [model outputs](#modeloutput) are used in model benchmarking visualizations. Only admin users can create experiment files from scratch. Registered users can clone existing experiments and edit certain fields for their own use. Please note that analysis scripts from cloned experiments cannot be edited at this stage. 

## Data  {#data}

Various data sets are used in a model comparison experiment. Only admin users can create data sets. All data sets are version controlled. Clicking on a data set name will take the user to the associated meta-data for a particular data set. Currently, there are many fluxnet tower data files for public use. Once a user downloads the data associated with an experiment and run their own simulation model locally, [model outputs](#modeloutput) will be produced.

## Model outputs and model profile {#modeloutput}

Model outputs are used to as inputs for the [analysis script](#analysisscript) in order to evaluate the performance of a model in a given experiment. A model profile which contains meta-data of the model outputs must be created first on `modelevaluation.org` before model outputs are uploaded on the site. During the upload process, a user can then select what benchmarks (e.g. other models uploaded by other users to the experiment) to compare with their own submission. Once the model outputs are submitted, the execution of the [analysis script](#analysisscript) associated with the experiment is automatically triggered.

## Analysis Script {#analysisscript}

The analysis script of a given experiment is created by an admin users and determines what type of visualizations are created for benchmarking across models. The analysis script is not limited to any programming language or package dependencies and accepts user supplied [model outputs](#modeloutput) as inputs. Once the analysis is complete, it will return the various images, metrics and evaluation products used, along with any error messages and addition information pertaining to the each image (e.g. filepaths and types).

## Evaluation product ?
