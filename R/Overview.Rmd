---
title: "An overview"
knit: (function(input_file, encoding) {
    out_dir <- '../docs';
    rmarkdown::render(input_file,
      encoding=encoding,
      output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We have provided a general overview of `modelevaluation.org` and described the function of each major component. We also recommend taking a look at our [step-by-step case study]() _[No link yet, an example study that is open access would be good!]_

## Access

To begin using `modelevaluation.org`, users must first register for an account. 
Click on `Register` on the top right hand corner and fill in the relevant information. Note that users must agree to the [Terms of Use](https://modelevaluation.org/terms-of-use) to create an account. Once registration is complete, click on  `Login` and enter your credentials to access `modelevaluation.org`. If you would like to have admin user rights, please contact us [here](mailto:admin@modelevaluation.org) 

## The Workflow

`modelevaluation.org` facilitates automated model evaluation as part of model development, as well as model comparisons, by hosting model evaluation experiments along with their associated data sets and analysis scripts in a centralized space. A registered user can download the data for a particular experiment, run their model simulation locally and upload their model output to modelevaluation.org see how their model performed relative to observational or reference data, as well other models that have contributed to the same experiment. As an example, the [PLUMBER land surface model inter-comparison experiment](https://journals.ametsoc.org/view/journals/hydr/16/3/jhm-d-14-0158_1.xml) was conducted though `modelevaluation.org`  

## Workspace

Any user can create a workspace in the web application. A workspace is an environment for model development and is used to group users together that are collaborating on the same project. A workshop stores the necessary [experiment](#experiment) and [datasets](#data) for a particular model comparison project. A workspace can be used privately or can be made public for larger scale model inter-comparisons. _[Want to link to a relevant paper]_

## Experiment {#experiment}

An experiment sets the key parameters (e.g data, types of analyses, sspatial and temporal scale) for a model evaluation project. It contains a [master analysis script](#analysisscript) which determines how [model outputs](#modeloutput) are analysed and which evaluation visualizations are produced. Only admin users can create new experiment templates, but any registered users can clone existing experiments from `modelevaluation.org` and edit their analysis scripts for their own use.

## Data  {#data}

Various data sets are used in a model comparison experiment. Only admin users can create data sets. All data sets are version controlled. Clicking on a data set name will take the user to the associated meta-dataand files for a particular data set. Currently, there are many flux tower data files for public use. Once a user downloads the data associated with an experiment and run their own simulation model locally, [model outputs](#modeloutput) will be produced.

## Model outputs and model profiles {#modeloutput}

Model outputs are uploaded by users to a particular experiment. They are used to as inputs for the [analysis script](#analysisscript) to evaluate the performance of models in a given experiment. A model profile contains meta-data about the model that created a particular output, and must be created first on `modelevaluation.org` before  outputs are uploaded. During the upload process, a user can select  nominating benchmarks (e.g. other outputs uploaded to the experiment by other users) to compare with their own submission. Once the model outputs are submitted, the execution of the [analysis script](#analysisscript) associated with the experiment is triggered.

## Analysis Script {#analysisscript}

The analysis script of a given experiment is created by an admin users and determines what type of visualizations are created for for evaluating  model outputs submitted to that experiment. The analysis script is not limited to any programming language or package dependencies and accepts user supplied [model outputs](#modeloutput) as inputs. Once the analysis is complete, it will typically return plots and diagnostics as images, along with any error messages and additional information pertaining to the each image (e.g. filepaths and types).

